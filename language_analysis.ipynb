{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbe3040",
   "metadata": {},
   "source": [
    "Author: Mateusz Burza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25788956",
   "metadata": {},
   "source": [
    "This notebook should be executed in Google Colab (online version).\n",
    "\n",
    "**Steps**\n",
    "1. Open: https://colab.research.google.com/\n",
    "2. Click **Upload notebook** and choose this file.\n",
    "3. Run `get_dicts_to_analysis.py` to generate:\n",
    "   - `wiki_big_article_wc.json`\n",
    "   - `wiki_low_conf_score_wc.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e702b",
   "metadata": {},
   "source": [
    "## Implementation of function `lang_confidence_score`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3a8c1",
   "metadata": {},
   "source": [
    "The `lang_confidence_score` function measures how closely an article’s word distribution matches a target language distribution. It first normalizes the article’s word counts, then computes the overlap of vocabularies and the average frequency difference on the shared words. These components are combined into an intermediate score, penalized when the overlap is small, and finally smoothed with a logistic function to return a value in $[0, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3059828",
   "metadata": {},
   "source": [
    "### Formulas used in `lang_confidence_score`\n",
    "Let $A$ be the set of unique article words and $L$ the set of words in an arbitrary language.\n",
    "Let $c_w$ be the count of word $w$ in the article and $L_w$ the language frequency of word $w$.\n",
    "\n",
    "**1. Normalization:**\n",
    "$$\n",
    "f_w = \\frac{c_w}{\\sum_u c_u}\n",
    "$$\n",
    "\n",
    "**2. Overlap and overlap ratio:**\n",
    "$$\n",
    "O = A \\cap L\n",
    "$$\n",
    "$$\n",
    "r = \\frac{|O|}{( |A| + |L| )/2}\n",
    "$$\n",
    "\n",
    "**3. Average frequency difference (with $\\varepsilon$):**\n",
    "$$\n",
    "\\Delta = \\frac{1}{|O|} \\sum_{w \\in O} \\left| f_w - L_w \\right|\n",
    "$$\n",
    "\n",
    "**4. Similarity:**\n",
    "$$\n",
    "s = \\max(0, 1 - \\Delta)\n",
    "$$\n",
    "\n",
    "**5. Overlap confidence penalty:**\n",
    "$$\n",
    "p = \\min\\left(1, \\frac{|O|}{2|L|}\\right) \\quad \\text{(or } p=1 \\text{ if } |O| > 1000\\text{)}\n",
    "$$\n",
    "\n",
    "**6. Intermediate score:**\n",
    "$$\n",
    "q = 0.4\\,r + 0.6\\,s\\,p\n",
    "$$\n",
    "\n",
    "**7. Logistic smoothing:**\n",
    "$$\n",
    "\\text{score} = \\frac{1}{1 + e^{-10(q-0.5)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_confidence_score(word_counts: dict,\n",
    "                            language_words_with_frequency: dict) -> float:\n",
    "    \"\"\"\n",
    "    Given a dictionary of word counts and a dictionary of language words\n",
    "    with their frequencies, return a confidence score in [0, 1]\n",
    "    indicating how well the text matches the language distribution.\n",
    "    \"\"\"\n",
    "    if not word_counts or not language_words_with_frequency:\n",
    "        return 0.0\n",
    "\n",
    "    total = sum(word_counts.values())\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # 1. Normalize `word_counts` to get frequencies\n",
    "    article_freqs = {w: c / total for w, c in word_counts.items()}\n",
    "\n",
    "    # Language frequencies as provided (assumed normalized by wordfreq)\n",
    "    lang_freqs = language_words_with_frequency\n",
    "    k = len(lang_freqs)\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # 2. Overlap ratio: how many words are common between article and language?\n",
    "    overlap = set(article_freqs.keys()) & set(lang_freqs.keys())\n",
    "    if not overlap:\n",
    "        return 0.0\n",
    "    \n",
    "    r = (len(article_freqs) + len(lang_freqs)) / 2\n",
    "    overlap_ratio = len(overlap) / r\n",
    "\n",
    "\n",
    "    # 3. Avg frequency difference: how well do article frequencies match language frequencies?\n",
    "    eps = 1e-9\n",
    "    diffs = [abs((article_freqs[w] + eps) - (lang_freqs[w] + eps)) for w in overlap]\n",
    "    avg_diff = sum(diffs) / len(diffs)\n",
    "    \n",
    "    # 4. Similarity: lower difference -> higher score\n",
    "    similarity = max(0.0, 1.0 - avg_diff)\n",
    "\n",
    "    # 5. Overlap confidence penalty: for small overlaps - avoid false positives with few common words\n",
    "    overlap_confidence = min(1.0, len(overlap) / (k * 5))\n",
    "    if len(overlap) > 1000:\n",
    "        overlap_confidence = 1.0\n",
    "\n",
    "    # 6. Combined score:\n",
    "    score = 0.4 * overlap_ratio + (0.6 * similarity * overlap_confidence)\n",
    "\n",
    "     # 7. Logistic smoothing:\n",
    "    from math import exp\n",
    "    logistic_score = 1.0 / (1.0 + exp(- 10 * (score - 0.5)))\n",
    "\n",
    "    return logistic_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f813f2c9",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce83f96",
   "metadata": {},
   "source": [
    "### Here we collect data from the wiki articles.\n",
    "\n",
    "You should upload the JSON files generated earlier by `get_dicts_to_analysis.py`:\n",
    "- `wiki_big_article_wc.json`\n",
    "- `wiki_low_conf_score_wc.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48755024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-e19dd3df-167c-43d1-9893-bb3d4f1c6a03\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-e19dd3df-167c-43d1-9893-bb3d4f1c6a03\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "from pathlib import Path\n",
    "from json import load\n",
    "\n",
    "big_path = Path(\"/content/wiki_big_article_wc.json\")\n",
    "low_path = Path(\"/content/wiki_low_conf_score_wc.json\")\n",
    "\n",
    "# appropriate dicts should be in these files\n",
    "uploaded = files.upload()\n",
    "\n",
    "with big_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    big_article_wc = load(f)\n",
    "\n",
    "with low_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    low_conf_score_wc = load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d5bf4",
   "metadata": {},
   "source": [
    "### Here we collect dara from `FrequencyWords`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb969c",
   "metadata": {},
   "source": [
    "This data will be used to compare results with wiki article word frequencies.\n",
    "\n",
    "**The code below produces three dictionaries:**\n",
    "- `external_wc[\"en\"]`\n",
    "- `external_wc[\"de\"]`\n",
    "- `external_wc[\"es\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfe25c5",
   "metadata": {},
   "source": [
    "Data source: FrequencyWords (https://github.com/hermitdave/FrequencyWords),\n",
    "licensed under CC BY-SA 4.0.\n",
    "\n",
    "Used for statistical analysis only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd20eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "def load_freq(lang, n=10_000):\n",
    "    \"\"\"Load frequency list from GitHub FrequencyWords repository\"\"\"\n",
    "    try:\n",
    "        url = f\"https://raw.githubusercontent.com/hermitdave/FrequencyWords/master/content/2018/{lang}/{lang}_50k.txt\"\n",
    "        df = read_csv(\n",
    "            url,\n",
    "            sep=\" \",\n",
    "            names=[\"word\", \"frequency\"]\n",
    "        )\n",
    "        return df.head(n)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading frequency data for {lang}: {e}\")\n",
    "        return None\n",
    "\n",
    "def normalize_to_dict(df):\n",
    "    if df is None:\n",
    "        return {}\n",
    "    total = df[\"frequency\"].sum()\n",
    "    return dict(zip(df[\"word\"], df[\"frequency\"] / total))\n",
    "\n",
    "langs = [\"en\", \"de\", \"es\"]\n",
    "\n",
    "external_wc = {}\n",
    "for lang in langs:\n",
    "    df = load_freq(lang)\n",
    "    external_wc[lang] = normalize_to_dict(df)\n",
    "    if external_wc[lang]:\n",
    "        print(f\"Loaded {len(external_wc[lang])} words for {lang}\")\n",
    "    else:\n",
    "        print(f\"Failed to load data for {lang}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f90685",
   "metadata": {},
   "source": [
    "### Here we collect data from `wordfreq`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e304d694",
   "metadata": {},
   "source": [
    "In this code cell I install `wordfreq`, import helpers, and define `build_language_freq()`.\n",
    "\n",
    "The function returns a dictionary of the top-$k$ words in the selected language with their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc58e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordfreq\n",
    "from wordfreq import top_n_list, word_frequency\n",
    "\n",
    "# Helper function: build dict of top-k words with their language frequencies\n",
    "def build_language_freq(language: str, k: int) -> dict:\n",
    "    words = top_n_list(language, k)\n",
    "    return {w: word_frequency(w, language) for w in words}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9abd06",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283d1f3",
   "metadata": {},
   "source": [
    "In the next cell I plot line charts of `lang_confidence_score` versus $k$ (top-$k$ words).\n",
    "\n",
    "There are three subplots, one per language (EN, ES, DE).\n",
    "\n",
    "Each subplot compares datasets (wiki and external) on the same $k$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [3, 10, 100, 1000, 2000, 3000, 4000, 4300, 4500, 4700, 4800, 5000]\n",
    "langs = [(\"English\", \"en\"), (\"Spanish\", \"es\"), (\"German\", \"de\")]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_counts_dict = {\n",
    "    \"low_conf\": low_conf_score_wc,\n",
    "    \"big_article\": big_article_wc,\n",
    "    \"external_en\": external_wc[\"en\"],\n",
    "    \"external_de\": external_wc[\"de\"],\n",
    "    \"external_es\": external_wc[\"es\"],\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(13, 5))\n",
    "\n",
    "for idx, (lang_name, lang_code) in enumerate(langs):\n",
    "    ax = axes[idx]\n",
    "    for label, wc in word_counts_dict.items():\n",
    "        scores = []\n",
    "        for k in k_values:\n",
    "            lang_freq = build_language_freq(lang_code, k)\n",
    "            s = lang_confidence_score(wc, lang_freq)\n",
    "            scores.append(s)\n",
    "        scores_for_plot = [s for s in scores]\n",
    "        ax.plot(k_values, scores_for_plot, marker=\"o\", label=label)\n",
    "    ax.set_title(f\"Lang. confidence vs k ({lang_name})\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"k values\")\n",
    "    ax.set_ylabel(\"Confidence\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87630a2",
   "metadata": {},
   "source": [
    "## Description of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17bcca",
   "metadata": {},
   "source": [
    "### Did the choice of languages have an impact on the confidence scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2b9dd",
   "metadata": {},
   "source": [
    "In the final version, the impact is small, as seen in the charts.\n",
    "\n",
    "However, English and German come from the same language family (Germanic) and originate from a geographically similar region. Although as $k$ increases, any mutual borrowings or popular common words become less significant.\n",
    "\n",
    "When `lang_confidence_score` is made less strict (e.g., $\\text{score} = \\frac{1}{1 + e^{-2(q-0.5)}}$),\n",
    "German performs slightly better than Spanish.\n",
    "\n",
    "In summary, the choice of languages has only a small impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758bcf05",
   "metadata": {},
   "source": [
    "### Looking at the `language_words_with_frequency` values for the data and the most frequent words in the language of the data, can your see that in the selected language words are often inflected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbf2c7",
   "metadata": {},
   "source": [
    "You can see that these words do not have many inflections (unlike, for example, in Polish).\n",
    "\n",
    "Something else stands out:\n",
    "- The vocabulary used in the wiki is quite niche.\n",
    "\n",
    "This is visible in the first chart: as $k$ increases, the `lang_confidence_score` function trends downward\n",
    "until about $k = 5000$, where it rises sharply.\n",
    "\n",
    "Why?\n",
    "- Larger $k$ increases the overlap, which makes $p = 1$.\n",
    "\n",
    "That means beyond the most popular words (often shared as loanwords), the wiki articles contain less common, domain-specific terms.\n",
    "Given the specificity of this wiki (Pokemon and the whole universe), this is expected.\n",
    "\n",
    "The second and third charts show that the overlap between the compared languages and the English‑wiki word sets differs noticeably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc1fb56",
   "metadata": {},
   "source": [
    "### Was it difficult to find and article for which the `lang_confidence_score` result is as low as possible in the wiki? Is this a specificity of this wiki?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c06e8f5",
   "metadata": {},
   "source": [
    "Yes and no.\n",
    "\n",
    "For the value of this function to be simply low? -> Yes.\n",
    "That mainly follows from what I described in the previous answer.\n",
    "\n",
    "For it to be the lowest possible? -> That took a bit of work.\n",
    "\n",
    "However, the differences were not huge. (maybe we need to add a new page to the wiki ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
